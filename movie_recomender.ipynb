{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY16URN0dzgO",
        "outputId": "cba64351-1970-44ae-bd45-24aaa72fd31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing movie dataset (first run may take a while)...\n",
            "Using Colab cache for faster access to the 'the-movies-dataset' dataset.\n",
            "Loaded 42277 movies.\n",
            "\n",
            "Enter some movie titles you like, separated by commas.\n",
            "Example: The Matrix, Inception, Toy Story\n",
            "Your liked movies: Spider-man, The Notebook, Twilight\n",
            "\n",
            "Because you like:\n",
            "  - Spider-man\n",
            "  - The Notebook\n",
            "  - Twilight\n",
            "\n",
            "You might also like:\n",
            "  - Spider-Man 3 (score: 0.270)\n",
            "  - Earth vs. the Spider (score: 0.212)\n",
            "  - Spider-Man 2 (score: 0.208)\n",
            "  - The Amazing Spider-Man 2 (score: 0.206)\n",
            "  - The Amazing Spider-Man (score: 0.205)\n",
            "  - Arachnophobia (score: 0.194)\n",
            "  - Spider-Plant Man (score: 0.189)\n",
            "  - Officer Down (score: 0.159)\n",
            "  - The Red Spider (score: 0.153)\n",
            "  - The Great Sinner (score: 0.151)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from ast import literal_eval\n",
        "from typing import List\n",
        "\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def download_movies_dataset() -> str:\n",
        "    \"\"\"\n",
        "    Download the 'rounakbanik/the-movies-dataset' from Kaggle using kagglehub\n",
        "    and return the local path where it is stored.\n",
        "    \"\"\"\n",
        "    path = kagglehub.dataset_download(\"rounakbanik/the-movies-dataset\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def _safe_literal_eval(val):\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    try:\n",
        "        return literal_eval(val)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return []\n",
        "\n",
        "\n",
        "def load_and_prepare_movies(dataset_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load movie data from the Kaggle dataset and create a 'metadata_text' column\n",
        "    suitable for building a content-based recommender.\n",
        "\n",
        "    Returns a DataFrame with at least:\n",
        "      - id\n",
        "      - title\n",
        "      - metadata_text\n",
        "    \"\"\"\n",
        "    movies_path = os.path.join(dataset_path, \"movies_metadata.csv\")\n",
        "    keywords_path = os.path.join(dataset_path, \"keywords.csv\")\n",
        "\n",
        "    movies = pd.read_csv(movies_path, low_memory=False)\n",
        "    keywords = pd.read_csv(keywords_path)\n",
        "\n",
        "    # Ensure IDs are numeric and align between files\n",
        "    movies[\"id\"] = pd.to_numeric(movies[\"id\"], errors=\"coerce\")\n",
        "    keywords[\"id\"] = pd.to_numeric(keywords[\"id\"], errors=\"coerce\")\n",
        "\n",
        "    movies = movies.dropna(subset=[\"id\"])\n",
        "    keywords = keywords.dropna(subset=[\"id\"])\n",
        "\n",
        "    # Merge keywords into movies\n",
        "    movies = movies.merge(keywords, on=\"id\", how=\"left\", suffixes=(\"\", \"_kw\"))\n",
        "\n",
        "    # Parse JSON-like fields\n",
        "    for col in [\"genres\", \"keywords\"]:\n",
        "        movies[col] = movies[col].apply(_safe_literal_eval)\n",
        "\n",
        "    def extract_names(items):\n",
        "        # Items are usually list[dict{name: \"...\"}]\n",
        "        if not isinstance(items, list):\n",
        "            return []\n",
        "        names = []\n",
        "        for it in items:\n",
        "            if isinstance(it, dict) and \"name\" in it:\n",
        "                names.append(it[\"name\"])\n",
        "        return names\n",
        "\n",
        "    movies[\"genres_str\"] = movies[\"genres\"].apply(lambda x: \" \".join(extract_names(x)))\n",
        "    movies[\"keywords_str\"] = movies[\"keywords\"].apply(lambda x: \" \".join(extract_names(x)))\n",
        "\n",
        "    # Some helpful text fields\n",
        "    for col in [\"overview\", \"tagline\"]:\n",
        "        if col not in movies.columns:\n",
        "            movies[col] = \"\"\n",
        "        movies[col] = movies[col].fillna(\"\")\n",
        "\n",
        "    # Build one combined text field\n",
        "    movies[\"metadata_text\"] = (\n",
        "        movies[\"title\"].fillna(\"\")\n",
        "        + \" \"\n",
        "        + movies[\"genres_str\"].fillna(\"\")\n",
        "        + \" \"\n",
        "        + movies[\"keywords_str\"].fillna(\"\")\n",
        "        + \" \"\n",
        "        + movies[\"overview\"].fillna(\"\")\n",
        "        + \" \"\n",
        "        + movies[\"tagline\"].fillna(\"\")\n",
        "    )\n",
        "\n",
        "    # Keep only useful columns\n",
        "    movies = movies[[\"id\", \"title\", \"metadata_text\"]].dropna(subset=[\"title\"])\n",
        "\n",
        "    # Remove potential duplicates on title, keeping the first occurrence\n",
        "    movies = movies.drop_duplicates(subset=[\"title\"])\n",
        "\n",
        "    return movies.reset_index(drop=True)\n",
        "\n",
        "\n",
        "class MovieRecommender:\n",
        "    \"\"\"\n",
        "    Content-based movie recommender using TF-IDF over movie metadata text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, movies_df: pd.DataFrame):\n",
        "        self.movies = movies_df.copy()\n",
        "        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=50000)\n",
        "        self.movie_matrix = self.vectorizer.fit_transform(self.movies[\"metadata_text\"])\n",
        "\n",
        "        # Map lowercased title to index\n",
        "        self.title_to_idx = {\n",
        "            title.lower(): idx for idx, title in enumerate(self.movies[\"title\"])\n",
        "        }\n",
        "\n",
        "    def recommend_from_likes(self, liked_titles: List[str], top_n: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Given a list of movie titles the user likes, return a DataFrame of recommended movies.\n",
        "        \"\"\"\n",
        "        if not liked_titles:\n",
        "            raise ValueError(\"liked_titles must contain at least one title.\")\n",
        "\n",
        "        indices = []\n",
        "        for t in liked_titles:\n",
        "            key = t.strip().lower()\n",
        "            if key in self.title_to_idx:\n",
        "                indices.append(self.title_to_idx[key])\n",
        "\n",
        "        if not indices:\n",
        "            raise ValueError(\n",
        "                \"None of the liked titles were found in the dataset. \"\n",
        "                \"Check spelling or try different titles.\"\n",
        "            )\n",
        "\n",
        "        # User profile as average of liked movie vectors\n",
        "        user_vec = np.asarray(self.movie_matrix[indices].mean(axis=0)) # Fix: Convert to numpy array\n",
        "\n",
        "        # Similarity to all movies\n",
        "        sims = cosine_similarity(user_vec, self.movie_matrix).flatten()\n",
        "\n",
        "        # Exclude liked movies themselves\n",
        "        for idx in indices:\n",
        "            sims[idx] = -1.0\n",
        "\n",
        "        top_indices = np.argsort(sims)[-top_n:][::-1]\n",
        "        results = self.movies.iloc[top_indices].copy()\n",
        "        results[\"similarity\"] = sims[top_indices]\n",
        "        return results[[\"title\", \"similarity\"]]\n",
        "\n",
        "\n",
        "def build_recommender() -> MovieRecommender:\n",
        "    \"\"\"\n",
        "    Convenience helper to:\n",
        "      1. Download the dataset (if needed)\n",
        "      2. Load and prepare the movies\n",
        "      3. Build and return a MovieRecommender instance\n",
        "    \"\"\"\n",
        "    dataset_path = download_movies_dataset()\n",
        "    movies_df = load_and_prepare_movies(dataset_path)\n",
        "    return MovieRecommender(movies_df)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Downloading and preparing movie dataset (first run may take a while)...\")\n",
        "    recommender = build_recommender()\n",
        "    print(f\"Loaded {len(recommender.movies)} movies.\")\n",
        "\n",
        "    print(\"\\nEnter some movie titles you like, separated by commas.\")\n",
        "    print(\"Example: The Matrix, Inception, Toy Story\")\n",
        "    user_input = input(\"Your liked movies: \").strip()\n",
        "\n",
        "    if not user_input:\n",
        "        print(\"No titles entered. Exiting.\")\n",
        "        return\n",
        "\n",
        "    liked_titles = [t.strip() for t in user_input.split(\",\") if t.strip()]\n",
        "\n",
        "    try:\n",
        "        recs = recommender.recommend_from_likes(liked_titles, top_n=10)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nBecause you like:\")\n",
        "    for t in liked_titles:\n",
        "        print(f\"  - {t}\")\n",
        "\n",
        "    print(\"\\nYou might also like:\")\n",
        "    for _, row in recs.iterrows():\n",
        "        print(f\"  - {row['title']} (score: {row['similarity']:.3f})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}